{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.optimizers import Adam\n",
        "import plotly.graph_objects as go\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Flatten, Dropout,Reshape\n",
        "from Bio import SeqIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "# example of training a gan on mnist\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D,MaxPooling1D\n",
        "from tensorflow.keras.layers import Convolution1D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "waoXOZGPykB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the FASTA file and extract the sequences"
      ],
      "metadata": {
        "id": "hvstCRpgzUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('C:/data/omicron800.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths}\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"0\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "# Convert sequences from strings to lists of integers\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "\n",
        "onehot_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "onehot_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        onehot_data[i, j] = onehot_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_file = 'X.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_file = 'y.npy'  # File name for the labels\n",
        "np.save(X_file, onehot_data)\n",
        "np.save(y_file, np.array(data['label']))\n",
        "\n",
        "omicron800=np.savez_compressed('omicron800.dat', x=onehot_data , y=np.array(data['label']))\n",
        "\n",
        "totalTrainData =np.load('omicron800.dat.npz',allow_pickle=True)\n",
        "\n",
        "x=totalTrainData['x']\n",
        "y=totalTrainData['y']\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "print(\"Shape of one-hot encoded data:\", onehot_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_file)\n",
        "print(\"y file saved as:\", y_file)\n"
      ],
      "metadata": {
        "id": "lQ72cvzCztbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "xtrain,xtest, ytrain,ytest=train_test_split(x, y, test_size=0.2)\n",
        "ytrain1=ytrain\n",
        "ytest1=ytest\n",
        "ytrain = to_categorical(ytrain1,1)\n",
        "ytest = to_categorical(ytest1,1)\n",
        "xtrain.shape"
      ],
      "metadata": {
        "id": "pKw9R5BWzws0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#define_discriminator"
      ],
      "metadata": {
        "id": "wvMDZ01R0Urr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_shape=(29912, 5)\n",
        "n_classes = 1\n",
        "\n",
        "def define_discriminator(in_shape=(29912, 5)):\n",
        "    # input\n",
        "    in_image = Input(shape=in_shape)\n",
        "\n",
        "    # feature extraction\n",
        "    fe = Conv1D(64, 5, padding='same', input_shape=in_shape[1:])(in_image)\n",
        "    fe = LeakyReLU(alpha=0.1)(fe)\n",
        "    fe = Conv1D(64, 5, padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.1)(fe)\n",
        "\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "\n",
        "    # dropout\n",
        "    fe = Dropout(0.2)(fe)\n",
        "\n",
        "    # output\n",
        "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\n",
        "    # define model\n",
        "    model = Model(in_image, out_layer)\n",
        "\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "oxFTJiBLz6j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#define_generator"
      ],
      "metadata": {
        "id": "qrAphXXA0c9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_generator(latent_dim):\n",
        "    in_lat = Input(shape=(latent_dim,))\n",
        "\n",
        "    # generator network\n",
        "    gen = Dense(320)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(29912 * 5)(gen)\n",
        "    gen = Reshape((29912, 5))(gen)\n",
        "\n",
        "    out_layer = Conv1D(5, 1, activation='tanh', padding='same')(gen)\n",
        "\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Cl_5aMoK0XHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(g_model, d_model):\n",
        "    # make weights in the discriminator not trainable\n",
        "    d_model.trainable = False\n",
        "\n",
        "    # get noise input from generator model\n",
        "    gen_noise = g_model.input\n",
        "\n",
        "    # get image output from the generator model\n",
        "    gen_output = g_model.output\n",
        "\n",
        "    # connect image output from generator as input to discriminator\n",
        "    gan_output = d_model(gen_output)\n",
        "\n",
        "    # define gan model as taking noise and outputting a classification\n",
        "    model = Model(gen_noise, gan_output)\n",
        "\n",
        "    # compile model\n",
        "    opt = Adam()\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bZva_-I50h-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample Generation Functions"
      ],
      "metadata": {
        "id": "o3E3Eat30y8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "    xdata, _ = dataset  # only images\n",
        "    ix = np.sort(randint(0, xdata.shape[0], n_samples))\n",
        "    X = xdata[ix]\n",
        "    y = ones((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    z_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return z_input\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    z_input = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(z_input)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return images, y\n"
      ],
      "metadata": {
        "id": "3sh4u2jd0jfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Function"
      ],
      "metadata": {
        "id": "lCduQxLZ03J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=100):\n",
        "    half_batch = int(n_batch / 2)\n",
        "    d_real_loss_epoch = []\n",
        "    d_fake_loss_epoch = []\n",
        "    g_loss_epoch = []\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        # get randomly selected 'real' samples\n",
        "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "        # update discriminator model weights\n",
        "        d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "        # generate 'fake' examples\n",
        "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "        # update discriminator model weights\n",
        "        d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "\n",
        "        # prepare points in latent space as input for the generator\n",
        "        z_input = generate_latent_points(latent_dim, n_batch)\n",
        "\n",
        "        y_gan = ones((n_batch, 1))\n",
        "        g_loss = gan_model.train_on_batch(z_input, y_gan)\n",
        "        g_loss_epoch.append(g_loss)\n",
        "        d_real_loss_epoch.append(d_loss1)\n",
        "        d_fake_loss_epoch.append(d_loss2)\n",
        "\n",
        "        # save the generator model every 10 epochs\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'>Epoch {i+1}, loss_real={d_loss1:.3f}, loss_fake={d_loss2:.3f}, loss_gan={g_loss:.3f}')\n",
        "            filename = f'cgan_model_{i + 1:03d}.h5'\n",
        "            g_model.save(filename)\n",
        "    return g_loss_epoch, d_real_loss_epoch, d_fake_loss_epoch\n"
      ],
      "metadata": {
        "id": "315vI1Ic0pd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = define_discriminator((29912, 5))\n",
        "print(d_model.summary())\n",
        "latent_dim = 100\n",
        "g_model = define_generator(latent_dim)\n",
        "print(g_model.summary())\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "print(gan_model.summary())\n"
      ],
      "metadata": {
        "id": "oGL90aYp0rGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "(g_loss_epoch,d_real_loss_epoch,d_fake_loss_epoch) = train(g_model, d_model, gan_model, (xtrain,ytrain), latent_dim,n_epochs=300, n_batch=200,n_classes=2)\n",
        "\n",
        "loss1 = pd.DataFrame({'fake' : d_fake_loss_epoch, 'real' : d_real_loss_epoch})\n",
        "\n",
        "headerList = ['fakeloss_x' ,'fakeloss_y' , 'realloss_x', 'realloss_y']\n",
        "\n",
        "loss1.to_csv('Loss1.csv')\n",
        "\n",
        "pd.read_csv('Loss1.csv')\n",
        "\n",
        "################generated facke sample\n",
        "latent_dim = 100\n",
        "n_samples = 1000\n",
        "\n",
        "# Load the trained generator model\n",
        "g_model = load_model('C:\\data\\cgan_model_100.h5')\n",
        "\n",
        "# Generate 20 fake samples\n",
        "generated_images, _ = generate_fake_samples(g_model, latent_dim, n_samples, 2)\n",
        "threshold = 0.8  # Example threshold value\n",
        "\n",
        "# Convert to binary values\n",
        "X_binary = np.where(generated_images >= threshold, 1, 0)\n",
        "\n",
        "\n",
        "# Example one-hot encoded DNA sequence\n",
        "X_train_onehot = X_binary  # Replace this with your actual X_train data\n",
        "\n",
        "# Define mapping from one-hot encoding to AGCTN\n",
        "agctn_mapping = {0: 'A', 1: 'G', 2: 'C', 3: 'T', 4: 'N'}\n",
        "\n",
        "# Convert one-hot encoded DNA sequence to AGCTN\n",
        "X_train_agctn = np.argmax(X_train_onehot, axis=-1)  # Get index of maximum value along last axis\n",
        "X_train_agctn = np.vectorize(agctn_mapping.get)(X_train_agctn)  # Map indices to AGCTN using dictionary\n",
        "\n",
        "print(X_train_agctn.shape)  # Shape of the converted AGCTN sequence\n",
        "print(X_train_agctn[0])  # Example converted AGCTN sequence\n",
        "\n",
        "####\n",
        "def save_fasta(sequence, file_path, sequence_name=\"sequence\"):\n",
        "    with open(file_path, \"w\") as f:\n",
        "        for i in range(sequence.shape[0]):\n",
        "            f.write(\">\" + sequence_name + \"_\" + str(i) + \"\\n\")  # Write sequence name with index to FASTA file\n",
        "            f.write(''.join(sequence[i]) + \"\\n\")  # Write sequence to FASTA file, joining AGCTN characters in each row\n",
        "\n",
        "\n",
        "file_path = \"C:\\data\\my_sequence_omicron.fasta\"  # Replace with desired file path\n",
        "save_fasta(X_train_agctn, file_path, sequence_name=\"X_train_agctn\")\n",
        "print(\"AGCTN sequence saved to FASTA file:\", file_path)\n"
      ],
      "metadata": {
        "id": "JT0CQh2R1AQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}