{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.optimizers import Adam\n",
        "import plotly.graph_objects as go\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Flatten, Dropout,Reshape\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import re\n",
        "from Bio import SeqIO\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "4jHXWwbA1oaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the FASTA file and extract the sequences"
      ],
      "metadata": {
        "id": "8x95xAlN57LX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A895_Kf51gwf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/my_sequence_omicron.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths}\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"0\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "# Convert sequences from strings to lists of integers\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "\n",
        "fake_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "fake_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        fake_data[i, j] = fake_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_file = 'X_fake.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_file = 'y_fake.npy'  # File name for the labels\n",
        "np.save(X_file, fake_data)\n",
        "np.save(y_file, np.array(data['label']))\n",
        "\n",
        "fakedata=np.savez_compressed('fake.dat', x=fake_data , y=np.array(data['label']))\n",
        "\n",
        "fakedata =np.load('fake.dat.npz',allow_pickle=True)\n",
        "\n",
        "x_fake=fakedata['x']\n",
        "y_fake=fakedata['y']\n",
        "\n",
        "print(x_fake.shape)\n",
        "print(y_fake.shape)\n",
        "\n",
        "print(\"Shape of one-hot encoded data:\", fake_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_file)\n",
        "print(\"y file saved as:\", y_file)\n",
        "\n",
        "#############eta one hot\n",
        "\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/omicron800.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths}\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"1\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "# Convert sequences from strings to lists of integers\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "\n",
        "real_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "real_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        real_data[i, j] = real_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_real = 'X_real.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_real = 'y_real.npy'  # File name for the labels\n",
        "np.save(X_real, real_data)\n",
        "np.save(y_real, np.array(data['label']))\n",
        "\n",
        "realdata=np.savez_compressed('real.dat', x=real_data , y=np.array(data['label']))\n",
        "\n",
        "realdata =np.load('real.dat.npz',allow_pickle=True)\n",
        "\n",
        "x_real=realdata['x']\n",
        "y_real=realdata['y']\n",
        "\n",
        "print(x_real.shape)\n",
        "print(y_real.shape)\n",
        "\n",
        "print(\"Shape of real data:\", real_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_real)\n",
        "print(\"y file saved as:\", y_real)\n",
        "\n",
        "\n",
        "####################################################\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/beta.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths}\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"2\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "# Convert sequences from strings to lists of integers\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "\n",
        "beta_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "beta_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        beta_data[i, j] = beta_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_file = 'X_beta.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_file = 'y_beta.npy'  # File name for the labels\n",
        "np.save(X_file, beta_data)\n",
        "np.save(y_file, np.array(data['label']))\n",
        "\n",
        "betadata=np.savez_compressed('beta.dat', x=beta_data , y=np.array(data['label']))\n",
        "\n",
        "betadata =np.load('beta.dat.npz',allow_pickle=True)\n",
        "\n",
        "x_beta=betadata['x']\n",
        "y_beta=betadata['y']\n",
        "\n",
        "print(x_beta.shape)\n",
        "print(y_beta.shape)\n",
        "\n",
        "print(\"Shape of one-hot encoded data:\", beta_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_file)\n",
        "print(\"y file saved as:\", y_file)\n",
        "\n",
        "\n",
        "#################alpha\n",
        "\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/alpha800.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths} # Only keep the first 700 sequences\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"3\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "\n",
        "# Convert sequences from strings to lists of integers\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "onehot_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "alpha_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        alpha_data[i, j] = onehot_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_alpha = 'X_alpha.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_alpha = 'y_alpha.npy'  # File name for the labels\n",
        "np.save(X_alpha, alpha_data)\n",
        "np.save(y_alpha, np.array(data['label'])) # Only keep the first 700 labels\n",
        "\n",
        "alphadata = np.savez_compressed('alpha.dat', x=alpha_data, y=np.array(data['label']))\n",
        "\n",
        "alphadata = np.load('alpha.dat.npz', allow_pickle=True)\n",
        "\n",
        "x_alpha = alphadata['x']\n",
        "y_alpha = alphadata['y']\n",
        "\n",
        "print(x_alpha.shape)\n",
        "print(y_alpha.shape)\n",
        "\n",
        "print(\"Shape of alpha:\", alpha_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_alpha)\n",
        "print(\"y file saved as:\", y_alpha)\n",
        "\n",
        "################delta\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/delta800.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths} # Only keep the first 700 sequences\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"4\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "\n",
        "# Convert sequences from strings to lists of integers\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "onehot_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "delta_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        delta_data[i, j] = onehot_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_delta = 'X_delta.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_delta = 'y_delta.npy'  # File name for the labels\n",
        "np.save(X_delta, delta_data)\n",
        "np.save(y_delta, np.array(data['label'])) # Only keep the first 700 labels\n",
        "\n",
        "deltadata = np.savez_compressed('delta.dat', x=delta_data, y=np.array(data['label']))\n",
        "\n",
        "deltadata = np.load('delta.dat.npz', allow_pickle=True)\n",
        "\n",
        "x_delta = deltadata['x']\n",
        "y_delta = deltadata['y']\n",
        "\n",
        "print(x_delta.shape)\n",
        "print(y_delta.shape)\n",
        "\n",
        "print(\"Shape of delta:\", delta_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_delta)\n",
        "print(\"y file saved as:\", y_delta)\n",
        "\n",
        "\n",
        "#########################\n",
        "# Read the FASTA file and extract the sequences\n",
        "with open('C:/data/gama1000.fasta') as fasta_file:\n",
        "    identifiers = []\n",
        "    lengths = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        identifiers.append(str(seq_record.seq))\n",
        "        lengths.append(len(seq_record.seq))\n",
        "\n",
        "d = {'sequences': identifiers, 'len': lengths} # Only keep the first 700 sequences\n",
        "data = pd.DataFrame(d)\n",
        "data['label'] = \"5\"\n",
        "\n",
        "allSeq = data.sequences\n",
        "\n",
        "# Convert sequences from strings to lists of integers\n",
        "nucleotides = ['A', 'G', 'C', 'T', 'N']\n",
        "allSeq_int = []\n",
        "for seq in allSeq:\n",
        "    seq_int = [nucleotides.index(base) for base in seq if base in nucleotides]\n",
        "    allSeq_int.append(seq_int)\n",
        "\n",
        "# Perform padding to a fixed length\n",
        "max_length = 29912  # Set the desired fixed length for sequences\n",
        "padded_sequences = pad_sequences(allSeq_int, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Initialize an empty array to store the one-hot encoded data\n",
        "onehot_dict = {nucleotide: np.eye(len(nucleotides))[i] for i, nucleotide in enumerate(nucleotides)}\n",
        "gamma_data = np.zeros((len(padded_sequences), max_length, len(nucleotides)), dtype=np.int8)\n",
        "\n",
        "# Perform one-hot encoding for each sequence in padded_sequences\n",
        "for i, seq in enumerate(padded_sequences):\n",
        "    for j, base in enumerate(seq):\n",
        "        gamma_data[i, j] = onehot_dict[nucleotides[base]]\n",
        "\n",
        "# Save the one-hot encoded data and the labels into separate X and y files\n",
        "X_gamma = 'X_gamma.npy'  # File name for the input data (one-hot encoded sequences)\n",
        "y_gamma = 'y_gamma.npy'  # File name for the labels\n",
        "np.save(X_gamma, gamma_data)\n",
        "np.save(y_gamma, np.array(data['label'])) # Only keep the first 700 labels\n",
        "\n",
        "gammadata = np.savez_compressed('gamma.dat', x=gamma_data, y=np.array(data['label']))\n",
        "\n",
        "gammadata = np.load('gamma.dat.npz', allow_pickle=True)\n",
        "\n",
        "x_gamma = gammadata['x']\n",
        "y_gamma =gammadata['y']\n",
        "\n",
        "print(x_gamma.shape)\n",
        "print(y_gamma.shape)\n",
        "\n",
        "print(\"Shape of gamma:\", gamma_data.shape)\n",
        "print(\"Shape of labels:\", np.array(data['label']).shape)\n",
        "print(\"X file saved as:\", X_gamma)\n",
        "print(\"y file saved as:\", y_gamma)\n",
        "\n",
        "####################concatenet\n",
        "\n",
        "x= np.concatenate((x_fake,x_real,x_alpha, x_delta, x_beta, x_gamma),axis=0)\n",
        "y=np.concatenate((y_fake, y_real,y_alpha, y_delta, y_beta, y_gamma),axis=0)\n",
        "totalData=np.savez_compressed('totaldata.dat', x=x , y=y)\n",
        "\n",
        "totalTrainData =np.load('totaldata.dat.npz',allow_pickle=True)\n",
        "\n",
        "x=totalTrainData['x']\n",
        "y=totalTrainData['y']\n",
        "\n",
        "\n",
        "####################################################\n",
        "x= np.concatenate((x_fake,x_real),axis=0)\n",
        "y=np.concatenate((y_fake, y_real),axis=0)\n",
        "totalData=np.savez_compressed('totaldata.dat', x=x , y=y)\n",
        "\n",
        "totalTrainData =np.load('totaldata.dat.npz',allow_pickle=True)\n",
        "\n",
        "x=totalTrainData['x']\n",
        "y=totalTrainData['y']\n",
        "\n",
        "#######################################################\n",
        "\n",
        "xtrain,xtest, ytrain,ytest=train_test_split(x, y, test_size=0.2)\n",
        "ytrain1=ytrain\n",
        "ytest1=ytest\n",
        "ytrain = to_categorical(ytrain1,6)\n",
        "ytest = to_categorical(ytest1,6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "mtcotge55-vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Define input shape and output dimensions - for each sample\n",
        "x_s = xtrain.shape[1:] # feature (x) shape (skip first sample axis [0])\n",
        "nc = ytrain.shape[1]   # number of classes (=1 without one-hot encoding)\n",
        "\n",
        "l_name='binary_crossentropy'\n",
        "a_name='binary_accuracy'\n",
        "\n",
        "model_f = Sequential(name='Nanog_CNN_1')\n",
        "model_f.add(Conv1D(filters=7, kernel_size=21,padding=\"same\", activation=\"relu\", input_shape=x_s))\n",
        "model_f.add(MaxPooling1D(pool_size=148))\n",
        "model_f.add(Dropout(0.5))\n",
        "model_f.add(Flatten())\n",
        "\n",
        "model_f.add(Dense(nc, activation='softmax'))\n",
        "\n",
        "#model_f.compile(optimizer='adam', loss=l_name, metrics=a_name)\n",
        "\n",
        "model_f.summary()\n",
        "\n",
        "adam= Adam(learning_rate=0.001)\n",
        "model_f.compile(loss ='CategoricalCrossentropy', optimizer = adam, metrics = ['Accuracy'])\n",
        "\n",
        "early_stopping_monitor = EarlyStopping(monitor='val_loss',patience = 2)\n",
        "history=model_f.fit(xtrain, ytrain,batch_size=100, epochs=100, validation_split=0.1,callbacks= early_stopping_monitor)\n",
        "model_f.save('model_varientofconcern.keras')\n"
      ],
      "metadata": {
        "id": "yKvsUbdN5Re8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['Accuracy'])\n",
        "plt.plot(history.history['val_Accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MUQrGPzT5VLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model_f.evaluate(xtest, ytest)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "ITp5aGsZ5Wzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "predictions = model_f.predict(xtest)\n",
        "\n",
        "y_test_arg=np.argmax(ytest,axis=1)\n",
        "Y_pred = np.argmax(predictions,axis=1)\n",
        "print(classification_report(y_test_arg, Y_pred))"
      ],
      "metadata": {
        "id": "ARoGs8Pd5Yfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "conf_matrix = metrics.confusion_matrix(y_test_arg,Y_pred)\n",
        "\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(conf_matrix,annot=True,ax=ax,fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\n",
        "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix');\n"
      ],
      "metadata": {
        "id": "5VbcVXfv5aDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_final = model_f.predict(xtest)\n",
        "\n",
        "pred_class = np.argmax(pred_final, axis=1)\n",
        "\n",
        "true_class = np.argmax(ytest,axis=1)\n"
      ],
      "metadata": {
        "id": "2DYg_B1J5bcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saliency"
      ],
      "metadata": {
        "id": "8e-PphR35usC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.utils.scores import CategoricalScore, BinaryScore\n",
        "from io import StringIO\n",
        "\n",
        "nuc2int = {\n",
        "    \"N\": 0,\n",
        "    \"A\": 1,\n",
        "    \"G\": 2,\n",
        "    \"C\": 3,\n",
        "    \"T\": 4,\n",
        "\n",
        "}\n",
        "\n",
        "int2nuc = dict((v, k) for k, v in nuc2int.items())\n",
        "\n",
        "\n",
        "# Create a Saliency instance with the modified model\n",
        "saliency = Saliency(model_f, model_modifier=ReplaceToLinear(), clone=True)\n",
        "\n",
        "xtest_float = xtest.astype('float32')\n",
        "\n",
        "s_id = range(1036)\n",
        "saliencies = []\n",
        "allScore = []\n",
        "for i in s_id:\n",
        "    print(i)\n",
        "    s_seq = xtest_float[i]\n",
        "    s_class = pred_class[i]\n",
        "    t_class = true_class[i]\n",
        "\n",
        "    score = CategoricalScore(s_class)\n",
        "    # allScore.append(score)\n",
        "\n",
        "    saliency2 = saliency(score, s_seq)\n",
        "    print(saliency2.shape)\n",
        "    saliencies.append(saliency2)\n",
        "\n",
        "saliencies = np.array(saliencies)\n",
        "print(\"Saliencies length\", len(saliencies))\n",
        "print(\"Saliency sequence shape\", saliencies.shape)\n",
        "\n",
        "print(\"s_class=\", s_class)\n",
        "print(\"t_class=\", t_class)\n",
        "\n",
        "\n",
        "\n",
        "#saliencies[saliencies<0.2]= 0\n",
        "\n",
        "sm= np.squeeze(saliencies, axis=None)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# create a heatmap of the saliency values\n",
        "fig, ax = plt.subplots(figsize=(20, 5))\n",
        "sns.heatmap(sm, cmap=\"coolwarm\", ax=ax)\n",
        "\n",
        "# add labels and title\n",
        "ax.set_title(\"Saliency heatmap for example sequence_total\")\n",
        "ax.set_xlabel(\"Nucleotide position\")\n",
        "ax.set_ylabel(\"One-hot encoded nucleotide\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l4alJt-Q5fbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest_float = ytest1.astype('object').astype('int8')\n",
        "\n",
        "\n",
        "y_fake_sal= np.where((ytest_float==0))\n",
        "print(y_fake_sal)\n",
        "y_real_sal= np.where((ytest_float==1))\n",
        "print(y_real_sal)\n",
        "\n",
        "y_beta_sal= np.where((ytest_float==2))\n",
        "print(y_beta_sal)\n",
        "\n",
        "y_alpha_sal= np.where((ytest_float==3))\n",
        "print(y_alpha_sal)\n",
        "y_delta_sal= np.where((ytest_float==4))\n",
        "print(y_delta_sal)\n",
        "y_gamma_sal= np.where((ytest_float==5))\n",
        "print(y_gamma_sal)"
      ],
      "metadata": {
        "id": "03tptMbf5hxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "saliency = Saliency(model_f, model_modifier=ReplaceToLinear(), clone=True)\n",
        "\n",
        "s_id =[   0,    1,    3,   11,   12,   14,   17,   19,   34,   72,   73,\n",
        "         75,   78,   86,   90,   98,   99,  112,  114,  120,  125,  133,\n",
        "        135,  138,  143,  147,  174,  177,  178,  182,  184,  185,  192,\n",
        "        197,  201,  207,  211,  213,  216,  226,  227,  236,  239,  242,\n",
        "        243,  245,  247,  250,  257,  261,  263,  266,  267,  268,  271,\n",
        "        275,  277,  283,  287,  291,  295,  297,  302,  313,  317,  324,\n",
        "        326,  329,  331,  336,  346,  347,  356,  358,  362,  366,  367,\n",
        "        395,  396,  399,  401,  402,  404,  406,  410,  414,  415,  416,\n",
        "        419,  426,  433,  436,  439,  443,  448,  456,  458,  461,  463,\n",
        "        472,  479,  482,  483,  491,  498,  499,  500,  506,  514,  522,\n",
        "        529,  532,  545,  550,  555,  556,  558,  561,  564,  565,  570,\n",
        "        575,  579,  581,  586,  587,  590,  592,  598,  609,  610,  620,\n",
        "        621,  643,  645,  652,  662,  663,  674,  675,  677,  678,  681,\n",
        "        682,  686,  687,  689,  691,  693,  695,  726,  734,  749,  754,\n",
        "        755,  757,  758,  759,  773,  781,  784,  793,  801,  805,  814,\n",
        "        819,  828,  841,  848,  853,  856,  864,  865,  873,  876,  895,\n",
        "        903,  905,  906,  920,  921,  922,  927,  931,  937,  941,  944,\n",
        "        948,  949,  973,  977,  980,  984,  987,  998, 1003, 1005, 1015,\n",
        "       1018, 1024, 1025]\n",
        "saliencies_betalabel = []\n",
        "allScore = []\n",
        "s_chr_f=[]\n",
        "for i in s_id:\n",
        "  print(i)\n",
        "  s_seq = xtest_float[i]\n",
        "  s_class = pred_class[i]\n",
        "  t_class = true_class[i]\n",
        "\n",
        "  s_int = np.argmax(s_seq,axis=1)            # convert to integer\n",
        "  s_chr = list(map(int2nuc.get, s_int))\n",
        "  s_chr_f.append(s_chr)\n",
        "  score = CategoricalScore(s_class)\n",
        "  #allScore.append(score)\n",
        "\n",
        "\n",
        "\n",
        "  saliency_beta = saliency(score, s_seq)\n",
        "  print(saliency_beta.shape)\n",
        "  saliencies_betalabel.append(saliency_beta)\n",
        "\n",
        "\n",
        "saliencies_betalabel = np.array(saliencies_betalabel)\n",
        "print(\"Saliencies length\", len(saliencies_betalabel))\n",
        "print(\"Saliency sequence shape\", saliencies_betalabel.shape)\n",
        "\n",
        "print (\"s_class=\" , s_class)\n",
        "print (\"t_class=\" , t_class)\n",
        "\n",
        "saliencies_betalabel[saliencies_betalabel<0.1]= 0\n",
        "sm_betalabel= np.squeeze(saliencies_betalabel, axis=None)\n",
        "\n",
        "# create a heatmap of the saliency values\n",
        "fig, ax = plt.subplots(figsize=(20, 5))\n",
        "sns.heatmap(sm_betalabel, cmap=\"coolwarm\", ax=ax)\n",
        "\n",
        "# add labels and title\n",
        "ax.set_title(\"Saliency heatmap for example gama\")\n",
        "ax.set_xlabel(\"Nucleotide position\")\n",
        "ax.set_ylabel(\"One-hot encoded nucleotide\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "######################################################################\n",
        "\n",
        "saliency = Saliency(model_f, model_modifier=ReplaceToLinear(), clone=True)\n",
        "\n",
        "s_id = [   8,    9,   20,   21,   26,   28,   37,   38,   44,   47,   48,\n",
        "         49,   51,   52,   57,   64,   66,   77,  102,  108,  115,  123,\n",
        "        124,  127,  130,  131,  144,  152,  160,  164,  168,  172,  205,\n",
        "        208,  209,  214,  241,  249,  253,  254,  256,  259,  262,  281,\n",
        "        286,  292,  298,  300,  305,  309,  325,  330,  333,  339,  340,\n",
        "        349,  359,  364,  381,  386,  389,  391,  393,  405,  407,  417,\n",
        "        418,  427,  428,  429,  471,  473,  474,  475,  480,  486,  494,\n",
        "        507,  513,  516,  517,  519,  520,  521,  527,  535,  547,  548,\n",
        "        553,  557,  566,  595,  596,  597,  605,  625,  630,  634,  638,\n",
        "        641,  647,  648,  654,  658,  664,  670,  683,  685,  692,  703,\n",
        "        712,  713,  717,  721,  737,  739,  744,  748,  775,  780,  787,\n",
        "        790,  796,  802,  816,  824,  832,  839,  849,  855,  857,  859,\n",
        "        861,  879,  881,  888,  899,  911,  913,  924,  925,  947,  967,\n",
        "        968,  972,  974,  981,  992, 1014, 1019, 1023, 1026, 1028, 1034]\n",
        "saliencies_betalabel = []\n",
        "allScore = []\n",
        "s_chr_f=[]\n",
        "for i in s_id:\n",
        "  print(i)\n",
        "  s_seq = xtest_float[i]\n",
        "  s_class = pred_class[i]\n",
        "  t_class = true_class[i]\n",
        "\n",
        "  s_int = np.argmax(s_seq,axis=1)            # convert to integer\n",
        "  s_chr = list(map(int2nuc.get, s_int))\n",
        "  s_chr_f.append(s_chr)\n",
        "  score = CategoricalScore(s_class)\n",
        "  #allScore.append(score)\n",
        "\n",
        "\n",
        "\n",
        "  saliency_beta = saliency(score, s_seq)\n",
        "  print(saliency_beta.shape)\n",
        "  saliencies_betalabel.append(saliency_beta)\n",
        "\n",
        "\n",
        "saliencies_betalabel = np.array(saliencies_betalabel)\n",
        "print(\"Saliencies length\", len(saliencies_betalabel))\n",
        "print(\"Saliency sequence shape\", saliencies_betalabel.shape)\n",
        "\n",
        "print (\"s_class=\" , s_class)\n",
        "print (\"t_class=\" , t_class)\n",
        "\n",
        "saliencies_betalabel[saliencies_betalabel<0.2]= 0\n",
        "sm_betalabel= np.squeeze(saliencies_betalabel, axis=None)\n",
        "\n",
        "# create a heatmap of the saliency values\n",
        "fig, ax = plt.subplots(figsize=(20, 5))\n",
        "sns.heatmap(sm_betalabel, cmap=\"coolwarm\", ax=ax)\n",
        "\n",
        "# add labels and title\n",
        "ax.set_title(\"Saliency heatmap for example sequence_real\")\n",
        "ax.set_xlabel(\"Nucleotide position\")\n",
        "ax.set_ylabel(\"One-hot encoded nucleotide\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "#####################################################\n",
        "saliency = Saliency(model_f, model_modifier=ReplaceToLinear(), clone=True)\n",
        "\n",
        "s_id = 6                             # select sequence id\n",
        "s_seq = xtest_float[s_id]                       # selected sequence (one hot encoded)\n",
        "s_int = np.argmax(s_seq,axis=1)            # convert to integer\n",
        "s_chr = list(map(int2nuc.get, s_int))      # map integer to DNA letters\n",
        "\n",
        "s_class = pred_class[s_id]  # predicted class for s_id\n",
        "t_class = true_class[s_id]  # true class for s_id\n",
        "\n",
        "score = CategoricalScore(s_class)\n",
        "sm_delta    = saliency(score, s_seq)      # saliency map for one sequence\n",
        "L_delta    = sm_delta.shape[1]                 # length of sequence\n",
        "\n",
        "print(s_class)\n",
        "print(t_class)\n",
        "\n",
        "sm_delta[sm_delta<0.3]= 0\n",
        "\n",
        "sm_saliency_delta=sm_delta[:, 26000:26550]\n",
        "print(sm_saliency_delta.shape)\n",
        "\n",
        "L_delta= sm_saliency_delta.shape[1]\n",
        "\n",
        "plt.figure(figsize=[100,20])\n",
        "barlist = plt.bar(np.arange(L_delta), sm_saliency_delta[0])\n",
        "#[barlist[i].set_color('C1') for i in range(5,1000)]  # Change the coloring here if you change the sequence index.\n",
        "plt.xlabel('Bases')\n",
        "plt.ylabel('Magnitude of saliency values')\n",
        "plt.xticks(np.arange(L_delta), list(s_chr[26000:26550]));\n",
        "#plt.title('sequence id: {} true label: {} pred label: {} '.format(s_id, t_class, s_class));\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################\n",
        "delta_code =np.load('delta.dat.npz',allow_pickle=True)\n",
        "\n",
        "x_delta=delta_code['x']\n",
        "y_delta=delta_code['y']\n",
        "\n",
        "x_delta[8][19970:19985]\n",
        "\n",
        "# Define mapping\n",
        "mapping = {'0': 'N', '1': 'A', '2': 'G', '3': 'C', '4':'T'}\n",
        "x_eta1= np.array([1, 1, 1, 4, 2, 3, 3, 3, 2, 4, 1, 1, 4, 2, 2], dtype=np.int8)\n",
        "# convert each sequence to letters\n",
        "for seq_int in x_eta1:\n",
        "    seq_int = [int(n) for n in str(seq_int)]  # convert integer sequence to list of integers\n",
        "    eta_encode = ''.join([mapping[str(num)] for num in seq_int])  # convert numbers to letters using mapping\n",
        "    eta = eta_encode\n",
        "    print(eta)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lLHlSnfU5MpM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}